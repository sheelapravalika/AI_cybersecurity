# -*- coding: utf-8 -*-
"""GradientBoostingClassifier

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D0azBevQiMmBPsi5cVSzH7OiTxLi5nR-
"""

!pip install -U datasets huggingface_hub fsspec pandas scikit-learn matplotlib seaborn numpy

# âœ… STEP 2: Import Libraries
from datasets import load_dataset
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import warnings
warnings.filterwarnings('ignore')

# âœ… STEP 3: Load CIC-IDS2017 (Binary) Dataset
from datasets import load_dataset
dataset = load_dataset("sonnh-tech1/cic-ids-2017", "binary")
df = dataset["train"].to_pandas()
print(df)

# âœ… STEP 4: Preprocess Data
#df.dropna(inplace=True)  # Drop missing rows
le = LabelEncoder()
df['Label'] = le.fit_transform(df['Label'])  # Benign=0, Threat=1
X = df.drop('Label', axis=1)
y = df['Label']
print(X,y)
X_numeric = X.select_dtypes(include=['int64', 'float64'])
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_numeric)
print(X_scaled)
print(y)

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split

# Split data (same as before)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

# Train the Gradient Boosting model
gb_model = GradientBoostingClassifier(n_estimators=10, learning_rate=0.05,subsample=0.8, max_depth=3, random_state=42)
gb_model.fit(X_train, y_train)

# Predict
y_pred_gb = gb_model.predict(X_test)

# Accuracy
print("âœ… Accuracy:", accuracy_score(y_test, y_pred_gb))

# Classification report
print("\nðŸ“„ Classification Report:\n", classification_report(y_test, y_pred_gb))

# Confusion Matrix
print("\nðŸ“Š Confusion Matrix:\n", confusion_matrix(y_test, y_pred_gb))

conf_mat_gb = confusion_matrix(y_test, y_pred_gb)
sns.heatmap(conf_mat_gb, annot=True, fmt='d', cmap='YlGnBu', xticklabels=le.classes_, yticklabels=le.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Gradient Boosting')
plt.show()

importances_gb = gb_model.feature_importances_
indices_gb = np.argsort(importances_gb)[-10:]

plt.figure(figsize=(10,6))
plt.title("Top 10 Feature Importances - Gradient Boosting")
plt.barh(range(len(indices_gb)), importances_gb[indices_gb], align='center')
plt.yticks(range(len(indices_gb)), [X.columns[i] for i in indices_gb])
plt.xlabel('Relative Importance')
plt.show()